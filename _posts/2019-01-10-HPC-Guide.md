---
layout: post
title: "High Performance Computer Guide"
subtitle: "Lichtenberg High Performance Computer of TU Darmstadt"
author: "Tianming Qiu"
lang: en
header-img: "img/post-bg-hpc-guide.jpeg"
header-mask: 0.3
mathjax: true
tags:

  - Software Engineering
  - Environment Configuration

---
## Get access
It is really exciting to use high performance computer, isn't it? But it looks a little bit different from the google cloud platform. It is usually called 'Cluster' which means many computers gathered together and it uses SLURM technology to arrange the tasks.
- You'd better attend the [introduction lecture](https://www.hhlr.tu-darmstadt.de/hhlr/startseite_news_details_hhlr_58432.en.jsp) given by TU Dramstadt computer center. It is usaully held monthly.
- **Account**: Check the [website](https://www.hhlr.tu-darmstadt.de/hhlr/zugang/nutzungsantrag/nutzungsrantrag.en.jsp) to see how to apply for the account. Usually you need to print and sign the document and give it to the secretary and they will help you to deliver it to the computer center. If you are a student or PhD candidate from TU Darmstadt, your Uni-ID will be your account. If you are a visitor, don't worry, you can also apply for a [TU guest ID](https://www.hrz.tu-darmstadt.de/id/tuid/index.en.jsp) and then apply for the account. 
- **Log-in**: `ssh -X -C tm34abcd@lcluster1.hrz.tu-darmstadt.de` here, `-X` means X11 forwarding, and `-C` means compression for speeding up.

## SLURM System
- [中科大教程](http://hmli.ustc.edu.cn/doc/userguide/slurm-userguide.pdf)

## File system
After log in successfully, you will see it is a typical linux system with 32 cores and huge memory(with the command line:`$ htop`). No matter which logging node you log in, the `/home` file will be a permanent one, the file will always be there. You can set up the virtual environment and install some softwares here just like on your own ubuntu systems. 
- /home
- /work/scratch/tm34abcd/: 10TB Quota, for files
- /work/projects
- /work/local
- More [details](https://www.hhlr.tu-darmstadt.de/hhlr/arbeit_auf_dem_cluster/dateisysteme_lichtenbergrechner_2/index.en.jsp)

## Configuration
- Actually you do not need to install python and lots of other stuff like git, cuda etc. on cluster.
- You can put `module load git python/3.6 cuda/9.0 openmpi/gcc` in '.bashrc' file.
- By the way, '.bashrc' file will work in all the situation, as well as the virtual environment. But '.bash_profile' will not.
- It will be quite different to install something on the cluster because you do not have the root access. So you can only install the software mannually through `wget`, such as [python install tutorials](https://www.godaddy.com/garage/how-to-install-and-configure-python-on-a-hosted-server/) and [MPI install guide](http://lsi.ugr.es/jmantas/pdp/ayuda/datos/instalaciones/Install_OpenMPI_en.pdf).
- To use vitual environment of python is a good habit: `virtualenv -p python3 envname`.
- Add this code into '.bashrc' file to highlight the directory path:
  ```sh
  # uncomment for a colored prompt, if the terminal has the capability; turned
  # off by default to not distract the user: the focus in a terminal window
  # should be on the output of commands, not on the prompt
  force_color_prompt=yes

  if [ -n "$force_color_prompt" ]; then
      if [ -x /usr/bin/tput ] && tput setaf 1 >&/dev/null; then
	  # We have color support; assume it's compliant with Ecma-48
	  # (ISO/IEC-6429). (Lack of such support is extremely rare, and such
	  # a case would tend to support setf rather than setaf.)
	  color_prompt=yes
      else
	  color_prompt=
      fi
  fi

  if [ "$color_prompt" = yes ]; then
      PS1='${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '
  else
      PS1='${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '
  fi
  unset color_prompt force_color_prompt
  ```
- **File transfer**: Notice that both command lines are implemented on your local PC.

  ```sh
  # Upload
  scp -r /home/ming/ tq56rola@lcluster6.hrz.tu-darmstadt.de:~/NormalDQN/
  # Download
  scp -r tq56rola@lcluster6.hrz.tu-darmstadt.de:~/NormalDQN/ /home/ming/
  ```

## Set up VIM
- [Vim Plugin](https://www.jianshu.com/p/f0513d18742a)
- [Color scheme](https://github.com/morhetz/gruvbox/wiki/Terminal-specific)

## Sbatch script:
You can run python as `python test.py`, but you cannot run the codes on the log-in nodes directly. You should send them through SLURM to the computational nodes via organizing your jobs in a sbatch script. Here is a script example:

```sh
#!/bin/bash

#SBATCH -J nvd8

#SBATCH -e /work/scratch/tq56rola/output/nvd8.err
#SBATCH -o /work/scratch/tq56rola/output/nvd8.out

#SBATCH -n 16
#SBATCH -t 5:00:00

#SBATCH --mem-per-cpu=1000
#SBATCH -C "nvd8"

# Load the required modules
module load gcc openmpi/gcc python/3.6 cuda/9.0

# Activate the virtualenv / conda environment
source ~/venv_py36/bin/activate

# cd into the working directory
cd ~/NormalDQN

python learning.py
```

## MPI framework:
You can organize mutiple tasks on different cores through this MPI framework which is related to distributing computing.
#### Do not be panic: a brief introduction
- two tutorials to read:
  - [Original Experiment Suite PDF](https://github.com/rueckstiess/expsuite/blob/master/documentation.pdf)
  - [Gregor's Git](https://github.com/gregorgebhardt/cluster_work)

#### Quick start
- Download Gregor's git and create a new python script which import cluster_work.

- When you work on the cluster, just `module load openmpi/gcc` and then `pip install mpi4py`. You do not need to install and MPI softwares in addition.

- Install yaml (`pip install PyYAML`) and mpi4py and cloudpickle. **Attention**: Create a pure py3.6 virtual  environment unless mpi4py in python 2.7 global will disturb the usage in the python 3.6. 

- Create your own 'yaml' file.

- Define your python sentences under 'iteration'. The return of the funtion 'iteration' will be saved under the default path.

- **Parameters passing**: config will be a dictionary which gets values from yaml file. So just write like `config['params']['b_value']`

- Add init function:
  ```python
  def __init__(self):
      ClusterWork.__init__(self)
      self.train_obj = DQNTrain()
  ```

- **Repetition and Iteration**: The repetition means it repeats your code again. And it will write a new `.csv` file. reset only works when a new repetition starts(also for the first round repetition),  not for a new iteration. The iteration parameters in `hyper-para.yml` will run your iteration function in learning_experiment for specific numbers. And you'd  better not writing loop in iterate function.

- The parameters in list will be assigned for different repetitions.

- <https://www.hhlr.tu-darmstadt.de/hhlr/betrieb/lichtenberg_software/lichtenberg_software_module/index.en.jsp>

- please specify the project with the “-A” option in your job scripts (e.g. #SBATCH –A project01032).

## Repeat sbatch script
- Parse the parameters through python
- Parse the parameters into sbatch: [example](http://www.voidcn.com/article/p-apuzwrcu-bvc.html)
- write the loop in [script](https://www.cyberciti.biz/faq/bash-for-loop/)
- 