---
layout: post
title: "Reinforcement Learning Notes"
subtitle: "Mixed Version"
author: "Tianming Qiu"
header-img: "img/post-bg-rl-note.jpg"
header-mask: 0.3
mathjax: true
tags:
  - RL
---

# Reinforcement Learning

## Reference books and novel papers
- OpenAI [key papers list](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)
- Neuro Dynamic Programming, Bertsekas & Tsitsiklis, 1996
- [Introduction to Reinforcement Learning](http://incompleteideas.net/book/bookdraft2017nov5.pdf), Sutton & Barto, 2017
- Markov Decision Problems, Puterman, 1994
- [Algorithms for Reinforcement Learning](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf), Szepesvari, 2010-2018.

## Small tips:
- **Mathematically formalize the RL problem**: $y=f(x)$ and $z$. Given state $x$ and reward $z$, try to generate policy or action $y$ for decisions making.

- **Bootstrapping method**(自举法/自助法): 通过对一个样本反复抽样，获得其分布。All of DP methods update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea *bootstrapping*. They learn a guess from a guess -- they bootstrap.

  TD learning methods are based on the recursive formulation of the Bellman equation, that is reformulated such that the prediction of the next value function is improved based on the current one. This strategy is known as bootstrapping.

- **TD learning and MC**: Sample updates differ from the expected updates of DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors.

- **Huber Loss**: 当预测偏差小于$\delta$时，它采用平方误差，当预测偏差大于δ的时候，采用的线性误差。相比于最小二乘的线性回归，Huber loss降低了对outliers的惩罚程度，所以Huber loss是一种常用的robust regression的损失函数。

- **Parametrized stochastic policy**: $\pi_{\mathbf{\theta}}(\mathbf{a}|\mathbf{s})$, where $\mathbf{\theta}$ is a is a parameter vector that specifies the policy. For example, if we have a Gaussian policy $\pi_{\mathbf{\theta}}(\mathbf{a}|\mathbf{s}) = \mathcal{N}(\mathbf{a}|\mu(s),\sigma^2)$, where the mean $\mu(\mathbf{s})$ is a linear combination of the state features $\phi(\mathbf{s})$, i.e $\mu(\mathbf{s}) = \phi(\mathbf{s})^T\mathbf{w}$. Then $\mathbf{\theta}$ corresponds to the parameters of the mean $\mathbf{w}$ and the variance $\sigma^2$ of the Gaussian distribution. Another common policy are neural networks. Then the parameters will be the weights and biases of the network.

- **Information gathering**: An exploration vs exploitation dilemma on the policy level between information gathering and exploitation.

- AC model没有采用Monte Carlo, In this approach the value function ("critic") is learned from the samples of the rollout and used for the update of the policy ("actor"), instead of directly using the Monte-Carlo sampled returns for the policy gradient equations. Furthermore, in DDPG a deterministic policy is used
  instead of a stochastic policy. The advantage of using a deterministic policy is that Silver et al. have extended the policy gradient theorem to deterministic policies, **which needs less samples**, since it only needs to integrate over the state space, while with stochastic policies it is necessary to integrate both over state and action space [66].

### **Bellman equation and Bellman optimality equation**
- *Bellman equation*:
  $$V^{\pi}(s) = R(s, \pi(s))+\gamma \sum_{s'}P(s'|s, \pi(s))V^{\pi}(s') $$
  This equation describes the expected reward for taking the action prescribed by some policy $\pi$. This has only one unique solution. Also a contraction mapping.

- *Bellman optimality equation*:
  $$V^*(s) = \underset{a}{\max}\big\{R(s, a)+\gamma \sum_{s'}P(s'|s, a)V^{*}(s') \big\}$$

  It describes the reward for taking the action giving the highest expected return.

- Alternative representation:

  $$V^{\pi}(\mathbf{s}) = \underset{\pi(\mathbf{a}|\mathbf{s})}{\mathbb{E}}\big[ r(\mathbf{s},\mathbf{a})+ \gamma\underset{\mathcal{P}(\mathbf{s}'|\mathbf{s}, \mathbf{a})}{\mathbb{E}}[V^{\pi}(\mathbf{s}')]\big]$$ 
  $$Q^{\pi}(\mathbf{s}, \mathbf{a}) = r(\mathbf{s},\mathbf{a})+ \gamma\underset{\mathcal{P}(\mathbf{s}'|\mathbf{s}, \mathbf{a}),\pi(\mathbf{a}|\mathbf{s})}{\mathbb{E}}\big[Q^{\pi}(\mathbf{s},\mathbf{a}')\big]$$
  $$V^{*}(\mathbf{s}) = \underset{\mathbf{a}}{\max}\big[ r(\mathbf{s},\mathbf{a})+ \gamma\underset{\mathcal{P}(\mathbf{s}'|\mathbf{s}, \mathbf{a})}{\mathbb{E}}[V^{*}(\mathbf{s}')]\big]$$ 


- Firstly, let's see what is TD-learning:
  $$Q^{\pi}_{new}(s_t,a_t) = Q^{\pi}_{old}(s_t, a_t)+\alpha\delta_t$$

  where $\delta_t$ is the TD error and defined as:

  $$\delta_t = r_t + \gamma Q^{\pi}_{old}(s+{t+1}, a')-Q^{\pi}_{old}(s_t,a_t)$$

  and can be described as the 1-step prediction error between the estimate of the current state and the next state.

- The difference between **Q-learning** and **SARSA** is which action $a'$ is used in the TD error. In SARSA one uses only collected samples from the policy, i.e., $a'=a_{t+1}$ , where $a_{t+1}$ is the next action that has been sampled in the next state $s_{t+1}$ of the same trajectory. Therefore, SARSA is referred as an **on-policy** algorithm. Q-learning instead uses actions that were not necessary generated by the policy and is considered as an **off-policy** algorithm for this reason. In Q-learning the highest Q is picked $a'=\arg\max_{a_{t+1}}Q(s_{t+1},a_{t+1})$.

- But as we don’t know the optimal Q-values nor the optimal policy and use only samples to find them, we need to use an stochastic exploration policy, since with a deterministic or greedy policy we may never visit relevant state-action pairs. Common exploration policies for the discrete action case are for example the epsilon greedy policy, which takes a random action with probability $\epsilon$ and with probability $1 −\epsilon$ the best action, or a softmax policy, based on the current Q-values. 

- While Q-learning will eventually approximate the state-value function of the optimal policy, SARSA will approximate the state-value function of the exploration policy. However, by reducing the exploration rate of the exploration policy SARSA will eventually converge to the optimal policy.

- $$Q(S_t, A_t) = Q(S_t, A_t) + \alpha[R(t) + \gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t)]$$
  $$Q(S_t, A_t) = Q(S_t, A_t) + \alpha[R(t) + \gamma \max_{a'} Q(S_{t+1}, a')-Q(S_t, A_t)]$$

- **ATTENTION !!!**: SARSA中的动作选取，不能保证$Q(S_{t+1}, A_{t+1})$的值最大；更何况，SARSA的执行策略可能还是$\epsilon$-greedy的。
  也就是说，所谓on/off-policy，即是指做TD更新时是否依赖当前的这个policy，Q-Learning不管你当前的这个policy怎么样，它“似乎”有一个知识库，就从过往的Q表中找最大的那个state-action value值，它不*rely on policy*.



- |          |  SARSA              | Q-Learning        	  |
  | :-------------- | :-------------- :| :-------------- :|
  | Choosing $A'$ | $\pi$ | $\pi$ |
  | Choosing $Q$ | $\pi$ | $\mu$ |

where $\pi$ is a $\epsilon$-greedy policy, and $\mu$ is a greedy policy.

### Bias and varaince
> Occam's razor is the problem-solving principle that the simplest solution tends to be the correct one. (William of Ockham)(Regularization)

- 首先明确一点，Bias和Variance是针对Generalization（一般化，泛化）来说的。
- 这个训练数据集的Loss与一般化的数据集（预测数据集）的Loss之间的差异就叫做Generalization error。而Generalization error又可以细分为Random Error、Bias和Variance三个部分。
#### 知乎简洁版：
- 给定一个数据集$\tau$，有唯一的真实模型$f$，但我们不知道。我们用不同的估计模型$\hat{f}$去逼近$f$，得到的均方误差MSE是不一样的，可以把这个MSE和$\hat{f}$都看作给定数据集$\tau $下的随机变量。 

  $MSE_{\tau} = \mathbb{E}_{\hat{f}|\tau}[(f-\hat{f})^2]=\mathbb{E}_{\hat{f}|\tau}[f^2-2f\hat{f}+\hat{f}^2]=f^2-2f\mathbb{E}_{\hat{f}|\tau}[\hat{f}]+\mathbb{E}_{\hat{f}|\tau}[\hat{f}]$（对于数据集$\tau $来说，真实模型$f$是确定的，所以对它求条件期望还是它自己）

  $=f^2-2f\mathbb{E}_{\hat{f}|\tau}[\hat{f}]+\mathbb{E}_{\hat{f}|\tau}^2[\hat{f}]-\mathbb{E}_{\hat{f}|\tau}^2[\hat{f}]+\mathbb{E}_{\hat{f}|\tau}[\hat{f}^2]$（凑完全平方项）

  $=(f-\mathbb{E}_{\hat{f}|\tau}[\hat{f}])^2+\mathrm{Var}_{\hat{f}|\tau}[\hat{f}]$（左边是完全平方项，右边恰好是方差公式$DX=\mathbb{E}X^2-\mathbb{E}^2X$） （其中即真实模型和估计模型的偏差bias的平方）

- 数据集中真实模型和估计模型的MSE被分解为两项，第一项就是模型偏差的平方，第二项就是估计模型的方差。

#### Jan Peters slides version:

- Estimator $\hat{f}_\mathcal{D}$ from training data $\mathcal{D}$, data generated by:
  $$y(\mathbf{x}_q) = f(\mathbf{x}_q)+\epsilon$$
  with $\mathbb{E}\{\epsilon\} = 0$ and $\mathrm{Var}\{\epsilon\}=\sigma_{\epsilon}^2$

- **Expected Squared Error** for query $\mathbf{x}_q$ estimated from all possible data sets $\mathcal{D}$:

  $$L_{\hat{f}}(\mathbf{x}_q) = \mathbb{E}_{\mathcal{D},\epsilon}\bigg \{ \big(y(\mathbf{x}_q)-\hat{f}_{\mathcal{D}}(\mathbf{x}_q)\big)^2\bigg\} = \mathbb{E}_{\mathcal{D},\epsilon}\bigg \{ \big(y(\mathbf{x}_q)-f(\mathbf{x}_q)+f(\mathbf{x}_q)-\hat{f}_{\mathcal{D}}(\mathbf{x}_q)\big)^2\bigg\}$$
  $$= \mathbb{E}_{\mathcal{D},\epsilon}\bigg \{ \underbrace{\big(y(\mathbf{x}_q)-f(\mathbf{x}_q)\big)^2}_{=\epsilon^2, \epsilon的平方全是正的，期望是\sigma^2}+\big(f(\mathbf{x}_q)-\hat{f}_{\mathcal{D}}(\mathbf{x}_q)\big)^2+2\underbrace{\big(y(\mathbf{x}_q)-f(\mathbf{x}_q) \big)\big( f(\mathbf{x}_q)-\hat{f}_{\mathcal{D}}(\mathbf{x}_q)\big)}_{\epsilon \big(f(\mathbf{x}_q)-\hat{f}_{\mathcal{D}}(\mathbf{x}_q) \big), \epsilon的期望为零} \bigg\}$$
  $$=\sigma^2_{\epsilon}+\mathbb{E}_{\mathcal{D}}\bigg \{ \big(f(\mathbf{x}_q)-\hat{f}_{\mathcal{D}}(\mathbf{x}_q) \big)^2\bigg \}$$ 其实这个第二项，就是一个error，可以拆为bias和varaince
  using $\bar{\hat{f}}(\mathbf{x}_q)=\mathbb{E}_{\mathcal{D}}\bigg\{\hat{f}_{\mathcal{D}}(\mathbf{x}_q) \bigg\}$, we obtain:
  $$\mathbb{E}_{\mathcal{D}}\bigg \{ \big(f(\mathbf{x}_q)-\hat{f}_{\mathcal{D}}(\mathbf{x}_q) \big)^2\bigg \}= \mathbb{E}_\mathcal{D}\bigg\{ \big( f(\mathbf{x}_q)-\bar{\hat{f}}(\mathbf{x}_q) +\bar{\hat{f}}(\mathbf{x}_q) -\hat{f}_{\mathcal{D}}(\mathbf{x}_q)\big)^2\bigg\} $$ 
  $$=\underbrace{\big( f(\mathbf{x}_q)-\bar{\hat{f}}(\mathbf{x}_q)\big)^2}_{\mathrm{bias}^2\{\hat{f}_{\mathcal{D}}(\mathbf{x}_q)\}} + \underbrace{\mathbb{E}_{\mathcal{D}}\bigg \{ \big(\hat{f}(\mathbf{x}_q)-\bar{\hat{f}}_{\mathcal{D}}(\mathbf{x}_q) \big)^2\bigg\}}_{=\mathrm{var}\{\hat{f}(\mathbf{x}_q)\}} $$
#### Monte Carlo and TD
- what are the different behaviors in B&V(???) of these two methods


### QMDP
- Thrun, Sebastian, Wolfram Burgard, and Dieter Fox. Probabilistic robotics. MIT press, 2005. P549, 16.2 QMDP
- Littman, Michael L., Anthony R. Cassandra, and Leslie Pack Kaelbling. "Learning policies for partially observable environments: Scaling up." Machine Learning Proceedings 1995. 1995. 362-370.
#### Fully observable MDP
- The simplest way to approximate teh value function for a POMDP is to assume that states of the process are fully observable.

- $\hat{V}(b) = \sum_{s\in \mathbb{S}}b(s)V^{*}_{MDP}(s)$ ,该式子是QMDP一个出发点，我的目标是计算$V(b)$，如果我知道$V(s)$，那么用概率来做加权平均，就可以得到最后的结果。**<u>A QMDP is computationally just about as efficient as an MDP, but it returns a policy that is defined over the belief state.</u>**

- 上式中的$V^{*}_{MDP}(s)$，是optimal value function for state $s$ for the fully observable version of the process. And it equals to:
  $$V_{i+1}^{MDP}(s)=\underset{a}{\max} \Big\{r(s,a)+\gamma\sum_{s'\in \mathbb{S}}P(s'|s,a)V_i^{MDP}(s')\Big \}$$

#### QMDP
- A variant of the approximate based on the fully observable MDP uses Q-functions:
  $$\hat{V}(b) = \underset{a\in\mathbb{A}}{\max}\sum_{s\in \mathbb{S}}b(s)Q^{*}_{MDP}(s)$$

  where,

  $$Q_{MDP}^{*}(s,a)=r(s,a)+\gamma\sum_{s'\in\mathbb{S}}P(s'|s,a)V_{MDP}^{*}(s')$$

- $$\hat{V}(b) = \underset{a\in\mathbb{A}}{\max}\sum_{s\in \mathbb{S}}b(s)\Big[r(s,a)+\gamma\sum_{s'\in\mathbb{S}}P(s'|s,a)\underset{\alpha_i\in\Tau_i}{\max}\alpha_i(s') \Big]$$，这里少乘了一个观测概率的矩阵以及对所有观测的求和，而这个求和小于1，所以，QMDP会产生一个高估的上界。

## Knowledge tree
### Lagrangian
- Jan Peters REPS, try to solve the problem via Lagrangian method, and the solution of Lagrangian multiplier is the representation of value function.
- Dual problem and primal problem.
### Convex optimiztion
- 

## Useful tutorials and web links:
- [Google cloud tutorials from CS231n](http://cs231n.github.io/gce-tutorial/)
- [关于优化的入门博客](https://yiyang186.github.io/categories/%E6%95%B0%E5%AD%A6/)
- [Git Virtual Env and Ignore control](https://www.cnblogs.com/heacool/p/6490640.html)