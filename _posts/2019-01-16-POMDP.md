---
layout: post
title: "POMDP Notes"
subtitle: 'Some planning methods and basic knowledges'
author: "Tianming Qiu"
header-style: text
mathjax: true
tags:
  - RL
  - POMDP
---

## $\alpha$-vectors
- $\pi$: any policy, whose actions are not conditioned on the initial belief

- $\vec{\alpha}$: length-$\vert s\vert$ vector like $[\alpha(s_1),\alpha(s_2),\dots ,\alpha(s_n)]$. $\alpha(s)$is the expected value of following $\pi$ starting from the state $s_0$.  
  $$
  \alpha(s) = \underset{\pi, s_0=s}{\mathbb{E}}\Big[  \sum_{t=0}^{h-1}\gamma^t R(s_t,a_t)\Big]
  $$

- Then the value of the executing $\pi$ starting from a belief $b$:  
  $$
  \begin{align}
  J_h\pi(b)&=\underset{\pi,b_0=b}{\mathbb{E}}\Big[\sum_{t=0}^{h-1}\gamma^t R(s_t,a_t) \Big]\\
  &=\underset{\pi,s_0\sim b}{\mathbb{E}}\Big[\sum_{t=0}^{h-1}\gamma^t R(s_t,a_t) \Big]\\
  &=\sum_s b(s)\underset{\pi,s_0=s}{\mathbb{E}}\Big[\sum_{t=0}^{h-1}\gamma^t R(s_t,a_t) \Big]\\
  &=\sum_s b(s)\alpha(s) \quad \text{for 2 states, a straight line}\\
  &=\mathbf{\vec{\alpha}}^T\mathbf{b}
  \end{align}
  $$
  so, we define that: $\mathbf{\alpha(b)}:=\mathbb{\vec{\alpha}}^T\mathbf{b}$.

- $\mathcal{T}_h=\{\pi_1,\pi_2,\dots,\pi_n\}$: all policy trees of depth $h$ as before.

  $\varGamma_h=\{\alpha_1,\alpha_2,\dots,\alpha_n\}$: corresponding set of $\alpha$ vectors.
  $$
  V^*(b)=\underset{\pi\in\mathcal{T}_h}{\max}J_h\pi(b):=\underset{\alpha\in\varGamma_h}{\max}\mathbf{\vec{\alpha}}^T\mathbf{b}
  $$

- In other words, for any finite $h$, the function $V_h^*$, can be written as the maximum of a finite number of linear functions. That is so-called '**Max-planes representation**'.






